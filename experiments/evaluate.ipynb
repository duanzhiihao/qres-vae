{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1fd0873b130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.functional as tvf\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_bits(obj):\n",
    "    return sys.getsizeof(pickle.dumps(obj)) * 8\n",
    "\n",
    "def evaluate_model(model, dataset_root):\n",
    "    device = next(model.parameters()).device\n",
    "    img_paths = Path(dataset_root).rglob('*.*')\n",
    "\n",
    "    all_image_stats = defaultdict(float)\n",
    "    pbar = tqdm(img_paths)\n",
    "    for impath in pbar:\n",
    "        # read image\n",
    "        im = tvf.to_tensor(Image.open(impath)).unsqueeze_(0).to(device=device)\n",
    "        # compression\n",
    "        compressed_obj = model.compress(im)\n",
    "        num_bits = get_object_bits(compressed_obj)\n",
    "        # decompression\n",
    "        im_hat = model.decompress(compressed_obj)\n",
    "        mse = torch.nn.functional.mse_loss(im, im_hat, reduction='mean')\n",
    "\n",
    "        # metrics\n",
    "        bpp  = float(num_bits / (im.shape[2] * im.shape[3]))\n",
    "        psnr = float(-10 * torch.log10(mse))\n",
    "        # logging\n",
    "        pbar.set_description(f'image {impath.stem}: bpp={bpp:.5f}, psnr={psnr:.3f}')\n",
    "        all_image_stats['bpp'] += bpp\n",
    "        all_image_stats['psnr']  += psnr\n",
    "        all_image_stats['count'] += 1\n",
    "\n",
    "    # average over all images\n",
    "    count = all_image_stats.pop('count')\n",
    "    results = {k: v/count for k,v in all_image_stats.items()}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lmb=16. Model weights=d:\\projects\\mycv-pytorch\\mycv\\weights\\my-vaes\\dh-64s4x\\dh_64s4x-lmb16\\last_ema.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image kodim24: bpp=0.28709, psnr=28.849: : 24it [00:07,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lmb=32. Model weights=d:\\projects\\mycv-pytorch\\mycv\\weights\\my-vaes\\dh-64s4x\\dh_64s4x-lmb32\\last_ema.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image kodim24: bpp=0.44863, psnr=30.903: : 24it [00:04,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lmb=64. Model weights=d:\\projects\\mycv-pytorch\\mycv\\weights\\my-vaes\\dh-64s4x\\dh_64s4x-lmb64\\last_ema.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image kodim24: bpp=0.64368, psnr=33.194: : 24it [00:04,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lmb=128. Model weights=d:\\projects\\mycv-pytorch\\mycv\\weights\\my-vaes\\dh-64s4x\\dh_64s4x-lmb128\\last_ema.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image kodim24: bpp=0.91211, psnr=35.330: : 24it [00:04,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lmb=256. Model weights=d:\\projects\\mycv-pytorch\\mycv\\weights\\my-vaes\\dh-64s4x\\dh_64s4x-lmb256\\last_ema.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image kodim24: bpp=1.25291, psnr=37.459: : 24it [00:04,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lmb=512. Model weights=d:\\projects\\mycv-pytorch\\mycv\\weights\\my-vaes\\dh-64s4x\\dh_64s4x-lmb512\\last_ema.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image kodim24: bpp=1.62880, psnr=39.515: : 24it [00:04,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lmb=1024. Model weights=d:\\projects\\mycv-pytorch\\mycv\\weights\\my-vaes\\dh-64s4x\\dh_64s4x-lmb1024\\last_ema.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image kodim24: bpp=2.14693, psnr=41.367: : 24it [00:04,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lmb=2048. Model weights=d:\\projects\\mycv-pytorch\\mycv\\weights\\my-vaes\\dh-64s4x\\dh_64s4x-lmb2048\\last_ema.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image kodim24: bpp=2.78479, psnr=43.288: : 24it [00:05,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpp    = [0.183965, 0.301801, 0.452579, 0.675982, 0.954627, 1.287195, 1.748591, 2.357312]\n",
      "psnr   = [30.01786, 31.97380, 33.88750, 35.84958, 38.14332, 40.22931, 42.21028, 44.31357]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mycv.models.vae.qres import qres34m\n",
    "\n",
    "dataset_root = 'd:/datasets/improcessing/kodak'\n",
    "from mycv.paths import MYCV_DIR\n",
    "weights_root = MYCV_DIR / 'weights/my-vaes/dh-64s4x'\n",
    "\n",
    "all_lmb_stats = defaultdict(list)\n",
    "for lmb in [16, 32, 64, 128, 256, 512, 1024, 2048]:\n",
    "    # initialize model\n",
    "    model = qres34m(lmb=lmb)\n",
    "\n",
    "    wpath = weights_root / f'dh_64s4x-lmb{lmb}/last_ema.pt'\n",
    "    msd = torch.load(wpath)['model']\n",
    "    model.load_state_dict(msd)\n",
    "\n",
    "    print(f'Evaluating lmb={lmb}. Model weights={wpath}')\n",
    "    model.compress_mode()\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    results = evaluate_model(model, dataset_root)\n",
    "    # print(results)\n",
    "    for k,v in results.items():\n",
    "        all_lmb_stats[k].append(v)\n",
    "\n",
    "for k, vlist in all_lmb_stats.items():\n",
    "    vlist_str = ', '.join([f'{v:.12f}'[:8] for v in vlist])\n",
    "    print(f'{k:<6s} = [{vlist_str}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "import json\n",
    "\n",
    "save_json_path = Path().cwd() / '../results/kodak-qres34m.json'\n",
    "with open(save_json_path, 'w') as f:\n",
    "    json.dump(all_lmb_stats, fp=f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd99ce241fc9a98adb16dacebaa58469dd0c84ca3cfa9b25e7e9cb4caa7bb934"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pt110env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
